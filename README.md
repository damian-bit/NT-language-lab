# ğŸ“– NT Language Lab

Comparador de traducciones del Nuevo Testamento usando IA local.

## ğŸ¯ DescripciÃ³n

Sistema de comparaciÃ³n lingÃ¼Ã­stica entre el texto original en griego koinÃ© y la traducciÃ³n Reina-Valera 1960 del Nuevo Testamento. Utiliza RAG (Retrieval-Augmented Generation) estricto para recuperar versÃ­culos especÃ­ficos y generar anÃ¡lisis lingÃ¼Ã­sticos usando un LLM local servido por **llama.cpp** (API HTTP tipo OpenAI), por ejemplo Phi-3 Mini Instruct en formato GGUF.

## ğŸ—ï¸ Arquitectura

Ver diagramas en [ARCHITECTURE.md](ARCHITECTURE.md).

- **Frontend**: Streamlit (puerto 8501)
- **Vector Store**: ChromaDB (persistencia en volumen Docker)
- **Embeddings**: sentence-transformers (paraphrase-multilingual-MiniLM-L12-v2)
- **LLM**: Servidor llama.cpp en el host (HTTP, API compatible OpenAI), p. ej. Phi-3 Mini Instruct (GGUF). **No** se ejecuta dentro de Docker.
- **Lenguaje**: Python 3.11

## ğŸ“‹ Requisitos Previos

1. **Docker y Docker Compose** instalados
2. **Servidor llama.cpp** ejecutÃ¡ndose en el host (puerto 8080 por defecto)
3. **Modelo GGUF** cargado en el servidor (p. ej. Phi-3 Mini Instruct)

## ğŸ“– InstalaciÃ³n y Uso Completo

### Paso 1: Servidor llama.cpp en el host

El backend LLM corre **fuera de Docker**. Debes tener ya:

- **llama.cpp** compilado con servidor HTTP (o un binario que exponga la API tipo OpenAI).
- **Modelo en GGUF** descargado (p. ej. Phi-3 Mini Instruct).

Inicia el servidor manualmente en el host, por ejemplo:

```bash
# Ejemplo: servidor en el puerto 8080
./server -m /ruta/al/modelo.gguf --port 8080
```

Verificar que el servidor responde:

```bash
curl http://localhost:8080/health
# Esperado: {"status":"ok"}
```

La app asume que el endpoint de chat estÃ¡ en `http://host.docker.internal:8080/v1/chat/completions` (configurable con `LLM_BASE_URL` y `LLM_MODEL`).

### Paso 2: Procesar los Datos

Los datos ya estÃ¡n disponibles localmente. Ejecuta el script de procesamiento para generar el JSON unificado:

```bash
python3 scripts/process_data.py
```

Este script:
- Lee `data/es_rvr/es_rvr.json` (Reina-Valera 1960)
- Lee los archivos TXT de `data/greek_nt/` (griego koinÃ©)
- Genera `data/nt_verses.json` con 7,925 versÃ­culos normalizados

**Nota**: Si ya ejecutaste este paso, puedes saltar al siguiente.

### Paso 3: Construir y Levantar los Contenedores

```bash
# Construir la imagen
docker-compose build

# Levantar el contenedor
docker-compose up -d
```

### Paso 4: Ingerir los Datos en ChromaDB

Ejecutar el script de ingesta dentro del contenedor:

```bash
docker-compose exec ntlanguagelab python scripts/ingest.py
```

Este proceso:
- Lee `data/nt_verses.json` (7,925 versÃ­culos)
- Genera embeddings con sentence-transformers
- Almacena en ChromaDB (persistente en `./chroma_db`)
- Tarda unos minutos (descarga el modelo de embeddings la primera vez)

**Nota**: La primera ejecuciÃ³n descargarÃ¡ el modelo de embeddings (~420MB). Las siguientes serÃ¡n mÃ¡s rÃ¡pidas.

### Paso 5: Acceder a la AplicaciÃ³n

Abre tu navegador en:

```
http://localhost:8501
```

## ğŸ“ Estructura del Proyecto

```
NTLanguageLab/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # Streamlit
â”‚   â”œâ”€â”€ rag.py               # RAG + ChromaDB
â”‚   â””â”€â”€ llm_client.py        # Cliente HTTP llama.cpp (OpenAI API)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ process_data.py      # RVR + griego â†’ nt_verses.json
â”‚   â””â”€â”€ ingest.py            # nt_verses.json â†’ ChromaDB
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ es_rvr/              # Reina-Valera 1960
â”‚   â”œâ”€â”€ greek_nt/            # Griego koinÃ© (MorphGNT)
â”‚   â””â”€â”€ nt_verses.json       # Generado por process_data.py
â”œâ”€â”€ chroma_db/               # ChromaDB (creado al ejecutar ingest)
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ ARCHITECTURE.md
```

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

El proyecto usa las siguientes variables (configuradas en `docker-compose.yml`):

- `LLM_BASE_URL`: URL base del servidor llama.cpp (por defecto: `http://host.docker.internal:8080`). Si estÃ¡ vacÃ­a o no definida, la app corre en modo solo RAG (sin comparaciÃ³n con IA).
- `LLM_MODEL`: Nombre del modelo en el servidor (por defecto: `Phi-3 Mini Instruct`)
- `CHROMA_DB_PATH`: Ruta donde se almacena ChromaDB (por defecto: `/app/chroma_db`)
- `DISABLE_LLM`: Si es `1`, `true` o `yes`, el LLM no se usa (modo solo RAG).

### VolÃºmenes Docker

- `./chroma_db` â†’ `/app/chroma_db`: Persistencia de ChromaDB
- `./data` â†’ `/app/data`: Datos del Nuevo Testamento

## ğŸ® Uso de la AplicaciÃ³n

Hay **dos modos de bÃºsqueda** (pestaÃ±as en la app):

- **Por referencia:** libro, capÃ­tulo y versÃ­culo (ej. Juan 3:16). Muestra griego + espaÃ±ol y, si el LLM estÃ¡ configurado, la comparaciÃ³n lingÃ¼Ã­stica se genera al instante.
- **Por concepto:** escribe una frase o concepto (ej. *amor de Dios al mundo*). La bÃºsqueda semÃ¡ntica devuelve versÃ­culos relacionados; eliges uno para ver detalle y, opcionalmente, generar la comparaciÃ³n con IA.

### Ejemplo: Buscar Juan 3:16

1. **Abre la aplicaciÃ³n**: `http://localhost:8501`
2. **Selecciona**:
   - Libro: "Juan"
   - CapÃ­tulo: `3`
   - VersÃ­culo: `16`
3. **Haz clic** en "ğŸ” Buscar y Comparar"
4. **VerÃ¡s**:
   - **Griego koinÃ©**: `ÎŸá½•Ï„Ï‰Ï‚ Î³á½°Ï á¼ Î³Î¬Ï€Î·ÏƒÎµÎ½ á½ Î¸Îµá½¸Ï‚ Ï„á½¸Î½ ÎºÏŒÏƒÎ¼Î¿Î½, á½¥ÏƒÏ„Îµ Ï„á½¸Î½ Ï…á¼±á½¸Î½ Ï„á½¸Î½ Î¼Î¿Î½Î¿Î³ÎµÎ½á¿† á¼”Î´Ï‰ÎºÎµÎ½...`
   - **Reina-Valera 1960**: `Porque de tal manera amÃ³ Dios al mundo, que ha dado a su Hijo unigÃ©nito...`
   - **ComparaciÃ³n lingÃ¼Ã­stica**: AnÃ¡lisis detallado generado por IA

### Otros Ejemplos

- **Mateo 1:1**: Libro de la genealogÃ­a
- **Romanos 8:28**: Todas las cosas ayudan a bien
- **1 Corintios 13:4**: El amor es sufrido

## ğŸ” CaracterÃ­sticas

- âœ… **Dos bÃºsquedas:** por referencia (libro/cap/vers) y por concepto (semÃ¡ntica).
- âœ… RAG estricto: Solo usa el versÃ­culo recuperado para la comparaciÃ³n
- âœ… ComparaciÃ³n lingÃ¼Ã­stica (no teolÃ³gica)
- âœ… Citas explÃ­citas de libro, capÃ­tulo y versÃ­culo
- âœ… IdentificaciÃ³n clara del idioma de cada texto
- âœ… Notas gramaticales cuando aplica
- âœ… Sin dependencias de APIs externas

## ğŸ› ï¸ Comandos Ãštiles

```bash
# Ver logs del contenedor
docker-compose logs -f

# Detener el contenedor
docker-compose down

# Reconstruir despuÃ©s de cambios
docker-compose up --build -d

# Acceder al shell del contenedor
docker-compose exec ntlanguagelab bash

# Verificar que el servidor llama.cpp responde
curl http://localhost:8080/health
```

## ğŸ“ Notas Importantes

1. **El servidor llama.cpp debe estar corriendo en el host** antes de usar la comparaciÃ³n con IA
2. **Los datos no estÃ¡n incluidos** - debes proporcionar tu propio archivo JSON
3. **El modelo GGUF debe estar cargado** en el servidor (p. ej. Phi-3 Mini Instruct)
4. **ChromaDB se crea automÃ¡ticamente** en `./chroma_db` al ejecutar la ingesta

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se puede conectar al LLM"

- Verifica que el servidor llama.cpp estÃ© corriendo: `curl http://localhost:8080/health`
- Comprueba `LLM_BASE_URL` y `LLM_MODEL` (variables de entorno o `docker-compose.yml`)
- En Linux, puede ser necesario ajustar `extra_hosts` en `docker-compose.yml`

### Error: "No se encontrÃ³ el versÃ­culo"

- Verifica que los datos hayan sido ingeridos correctamente
- Revisa el formato del archivo JSON
- Verifica que el libro, capÃ­tulo y versÃ­culo existan en tus datos

### ChromaDB no persiste

- Verifica que el volumen `./chroma_db` tenga permisos de escritura
- Revisa los logs: `docker-compose logs ntlanguagelab`

## ğŸš€ Publicar el proyecto (GitHub)

1. **Crear un repositorio** en GitHub (nuevo, vacÃ­o, pÃºblico o privado).
2. **No subas** `chroma_db/` ni archivos pesados: estÃ¡n en `.gitignore`. Los datos (`data/`) sÃ­ pueden subirse si no son enormes; si prefieres no versionarlos, aÃ±ade `data/nt_verses.json` (y opcionalmente `data/es_rvr/`, `data/greek_nt/`) a `.gitignore`.
3. **En la raÃ­z del proyecto** (donde estÃ¡ `docker-compose.yml`), ejecuta:
   ```bash
   git init
   git add .
   git commit -m "Initial commit: NT Language Lab"
   git branch -M main
   git remote add origin https://github.com/TU_USUARIO/NTLanguageLab.git
   git push -u origin main
   ```
   (Sustituye `TU_USUARIO/NTLanguageLab` por tu usuario y nombre del repo.)
4. **Opcional:** AÃ±ade una licencia (p. ej. MIT) creando `LICENSE` y un breve `CONTRIBUTING.md` si quieres aceptar contribuciones.
5. Quien clone el repo podrÃ¡ seguir el README: clonar, `docker-compose build`, `docker-compose up -d`, ejecutar ingest y usar la app. Para modo solo RAG (sin LLM), puede poner `DISABLE_LLM=1` en el entorno o en `docker-compose.yml`.

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto. Los datos del Nuevo Testamento deben obtenerse de fuentes pÃºblicas con las licencias correspondientes.
